{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce2cd163",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fancyimpute in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: scikit-learn>=0.24.2 in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from fancyimpute) (1.1.3)\n",
      "Requirement already satisfied: nose in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from fancyimpute) (1.3.7)\n",
      "Requirement already satisfied: pytest in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from fancyimpute) (7.1.1)\n",
      "Requirement already satisfied: cvxpy in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from fancyimpute) (1.2.2)\n",
      "Requirement already satisfied: knnimpute>=0.1.0 in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from fancyimpute) (0.1.0)\n",
      "Requirement already satisfied: cvxopt in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from fancyimpute) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.10 in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from knnimpute>=0.1.0->fancyimpute) (1.21.5)\n",
      "Requirement already satisfied: six in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from knnimpute>=0.1.0->fancyimpute) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24.2->fancyimpute) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24.2->fancyimpute) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24.2->fancyimpute) (2.2.0)\n",
      "Requirement already satisfied: ecos>=2 in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from cvxpy->fancyimpute) (2.0.10)\n",
      "Requirement already satisfied: scs>=1.1.6 in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from cvxpy->fancyimpute) (3.2.2)\n",
      "Requirement already satisfied: osqp>=0.4.1 in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from cvxpy->fancyimpute) (0.6.2.post8)\n",
      "Requirement already satisfied: qdldl in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from osqp>=0.4.1->cvxpy->fancyimpute) (0.1.5.post2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from pytest->fancyimpute) (21.4.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from pytest->fancyimpute) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from pytest->fancyimpute) (21.3)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from pytest->fancyimpute) (1.0.0)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from pytest->fancyimpute) (1.11.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from pytest->fancyimpute) (1.2.2)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from pytest->fancyimpute) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from pytest->fancyimpute) (0.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\nzahedin\\anaconda3\\lib\\site-packages (from packaging->pytest->fancyimpute) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af42449e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\TEMP\\ipykernel_16968\\1667501529.py:2: DtypeWarning: Columns (10,12,13,14,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('MNP_12_10.csv')\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file using the pandas library\n",
    "df = pd.read_csv('MNP_12_10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa62efb",
   "metadata": {},
   "source": [
    "## Data Cleaning:\n",
    "\n",
    "- It is important to understand why these values are missing and if there is a pattern in the missing values. For example, if the missing values are present only in a particular subset of the data, it could be that these values are missing for a reason and imputing them with the mode strategy might not be the best approach.\n",
    "\n",
    "- In addition to imputing the missing values, it might also be a good idea to perform some feature engineering to create new features that can help the autoencoder learn better.\n",
    "\n",
    "- Once the missing values have been imputed and any new features have been created, it is a good idea to perform some data visualization to understand the distribution of the data and identify any potential outliers. This can help us identify any data points that might be anomalies, and it can also help us decide on the appropriate scaling and normalization techniques for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59a026e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the columns except those with the object datatype\n",
    "df = df.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15030527",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 BoardNotifiedDate\n",
      "1 BusinessDate\n",
      "2 ServiceFromDate\n",
      "3 ServiceToDate\n",
      "4 Cost\n",
      "5 ExpenseSubgroup\n",
      "6 ExpenseItem\n",
      "7 PayeeCategory\n",
      "8 PayeeSubcategory\n",
      "9 Payee\n",
      "10 PayeeKey\n",
      "11 ClinicName\n",
      "12 ProviderNo\n",
      "13 TariffCode\n",
      "14 TariffDescription\n",
      "15 TariffFee\n",
      "16 WcbServiceCode\n",
      "17 WcbServiceCategory\n",
      "18 WcbService\n",
      "19 Units\n",
      "20 DrugQuantity\n",
      "21 AdjudicationStatus\n",
      "22 WageLossClaimType\n",
      "23 InjuryType\n",
      "24 SpecialInvestigationsFlag\n",
      "25 ICDCode\n",
      "26 InjuryBodyPartGroup\n",
      "27 InjuryBodyPart\n",
      "28 InjuryEventGroup\n",
      "29 InjuryEvent\n",
      "30 InjuryNatureGroup\n",
      "31 InjuryNature\n",
      "32 MSI\n",
      "33 PsychologicalCondition\n",
      "34 InjurySourceGroup\n",
      "35 InjurySource\n",
      "36 stickmanGroup\n",
      "37 stickmanSubGroup\n",
      "38 stickman\n"
     ]
    }
   ],
   "source": [
    "# delet the following columns V42 to V107\n",
    "df = df.drop(df.columns[39:105], axis=1)\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "     print(i, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04163baa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BoardNotifiedDate': 0.0,\n",
       " 'BusinessDate': 0.0,\n",
       " 'ServiceFromDate': 0.0,\n",
       " 'ServiceToDate': 0.0,\n",
       " 'Cost': 0.0,\n",
       " 'ExpenseSubgroup': 0.0,\n",
       " 'ExpenseItem': 0.0,\n",
       " 'PayeeCategory': 0.0,\n",
       " 'PayeeSubcategory': 0.0,\n",
       " 'Payee': 0.0,\n",
       " 'PayeeKey': 0.0,\n",
       " 'ClinicName': 0.0,\n",
       " 'ProviderNo': 0.0,\n",
       " 'TariffCode': 0.0,\n",
       " 'TariffDescription': 0.0,\n",
       " 'TariffFee': 0.0,\n",
       " 'WcbServiceCode': 0.002088275128679083,\n",
       " 'WcbServiceCategory': 0.005908410888978454,\n",
       " 'WcbService': 0.012405466098256528,\n",
       " 'Units': 0.023241363449088094,\n",
       " 'DrugQuantity': 0.06571969501678734,\n",
       " 'AdjudicationStatus': 0.12498178699504023,\n",
       " 'WageLossClaimType': 0.20187003289324737,\n",
       " 'InjuryType': 0.2707902852616105,\n",
       " 'SpecialInvestigationsFlag': 0.37073922159970485,\n",
       " 'ICDCode': 0.42783368578766934,\n",
       " 'InjuryBodyPartGroup': 0.47839702243707716,\n",
       " 'InjuryBodyPart': 0.42797938982734746,\n",
       " 'InjuryEventGroup': 0.3520509973105276,\n",
       " 'InjuryEvent': 0.27950159355387394,\n",
       " 'InjuryNatureGroup': 0.22945113512413312,\n",
       " 'InjuryNature': 0.20304687321372453,\n",
       " 'MSI': 0.19416789319580066,\n",
       " 'PsychologicalCondition': 0.18150822958832108,\n",
       " 'InjurySourceGroup': 0.1688848799107305,\n",
       " 'InjurySource': 0.22034306352382305,\n",
       " 'stickmanGroup': 0.1540970406837061,\n",
       " 'stickmanSubGroup': 0.18413359222325984,\n",
       " 'stickman': 0.17312553994554702}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the percentage of missing values in the dataset?\n",
    "def get_missing_percentage(df):\n",
    "    missing_percentage = {}\n",
    "    for col in df.columns:\n",
    "        missing_percentage[col] = df[col].isnull().sum() / df.shape[0]\n",
    "    return missing_percentage\n",
    "\n",
    "\n",
    "# get the percentage of missing values for different columns in the dataset.\n",
    "missing_percentage = get_missing_percentage(df)\n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "68d903ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute the missing values with mode strategy.\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ff2376",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Some potential feature engineering approaches that you could try are:\n",
    "\n",
    "- Derive new features from the dates columns (BoardNotifiedDate, BusinessDate, ServiceFromDate, ServiceToDate) such as the difference between ServiceFromDate and ServiceToDate to calculate the duration of the service, or the difference between BoardNotifiedDate and BusinessDate to calculate the time lag between the two dates.\n",
    "\n",
    "- Create new features based on the string columns such as the length of the string in the Payee column, or the number of unique words in the TariffDescription column.\n",
    "\n",
    "- Extract numerical features from the string columns such as the numbers present in the Cost or TariffFee columns.\n",
    "\n",
    "- Group the columns based on their categories and create new features that represent the relationships between the different categories. For example, you could create a new feature that represents the relationship between the InjuryType and InjuryBodyPart columns, or the relationship between the InjuryNatureGroup and InjuryNature columns.\n",
    "\n",
    "- Use some advanced feature engineering techniques such as text embeddings to represent the string columns in a more compact and meaningful way. This could be useful for columns such as TariffDescription or InjuryEvent that contain a large number of unique words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f827bc",
   "metadata": {},
   "source": [
    "## AutoEnncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1615b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder network\n",
    "inputs = tf.keras.Input(shape=(data.shape[1],))\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(inputs)\n",
    "encoded = tf.keras.layers.Dense(64, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(32, activation='relu')(encoded)\n",
    "\n",
    "# Define the decoder network\n",
    "decoded = tf.keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
